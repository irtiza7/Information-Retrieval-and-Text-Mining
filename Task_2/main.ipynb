{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bV8aJD9yo7CD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(input_file_path):\n",
    "    output_file_path = \"./Stopwords_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                if l_word in NLTK_STOP_WORDS:\n",
    "                    words.remove(word)\n",
    "\n",
    "            output_file.write(\" \".join(words))\n",
    "            output_file.write(\" \")\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_file_path):\n",
    "    output_file_path = \"./Punctuation_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w\") as output_file:\n",
    "        data = input_file.read()\n",
    "        for char in chars_to_remove:\n",
    "            data = data.replace(str(char), \"\")\n",
    "        \n",
    "        output_file.write(data)\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(input_file_path):\n",
    "    output_file_path = \"./Lowercased_Documents/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            lowercased_words = []\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                lowercased_words.append(l_word)\n",
    "\n",
    "            output_file.write(\" \".join(lowercased_words))\n",
    "            output_file.write(\" \")\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_content():\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    docs_with_content = {}\n",
    "    \n",
    "    for input_path in tqdm_notebook(docs_paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "        doc_words = []\n",
    "        with open(input_path, \"r\") as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            for line in lines:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    doc_words.append(word)\n",
    "        \n",
    "        docs_with_content[os.path.basename(input_path)] = doc_words\n",
    "\n",
    "    return docs_with_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(documents):\n",
    "    vocabulary = {}\n",
    "    for doc_words in tqdm_notebook(documents.values(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        for word in doc_words: \n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = True\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documnet_term_frequency(documents_content):\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    doc_term_frequency = {}\n",
    "    \n",
    "    for input_path in docs_paths:\n",
    "        doc_name = os.path.basename(input_path)\n",
    "        doc_term_frequency[doc_name] = {}\n",
    "        \n",
    "    for doc in tqdm_notebook(documents_content.keys(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        current_doc_words = documents_content[doc]\n",
    "        words_count = {}\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] = 0\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] += 1\n",
    "        \n",
    "        doc_term_frequency[doc] = words_count\n",
    "    \n",
    "    return doc_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_document_frequency(vocabulary, document_tf):\n",
    "    word_df = {}\n",
    "    for vocab_word in tqdm_notebook(vocabulary.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        word_doc_count = 0\n",
    "        for tf in document_tf.values():\n",
    "            if vocab_word in tf:\n",
    "                word_doc_count += 1\n",
    "                \n",
    "        word_df[vocab_word] = word_doc_count\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(vocab, m, word_df):\n",
    "    inverse_df_vocab = {}\n",
    "    for vocab_word in tqdm_notebook(vocab.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        inverse_df_vocab[vocab_word] = np.log2((m + 1) / word_df[vocab_word])\n",
    "    \n",
    "    return inverse_df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(idf_vocab, doc_tf):\n",
    "    tf_idf_docs = {}\n",
    "    for doc_name in doc_tf.keys():\n",
    "        tf_idf_docs[doc_name] = {}\n",
    "        \n",
    "    for (doc_name, words_freq) in tqdm_notebook(doc_tf.items(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        tf_idf_words = {}\n",
    "        for (doc_word, word_freq) in words_freq.items():\n",
    "            tf_idf_words[doc_word] = word_freq * idf_vocab[doc_word]\n",
    "            \n",
    "        tf_idf_docs[doc_name] = tf_idf_words\n",
    "        \n",
    "    return tf_idf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_space_model(query, tf_idf_docs):\n",
    "    query_word_count = {}\n",
    "    relevance_scores = {}\n",
    "    \n",
    "    for word in query:\n",
    "        query_word_count[word] = 0\n",
    "        \n",
    "    for word in query:\n",
    "        query_word_count[word] += 1\n",
    "    \n",
    "    for doc_name in tf_idf_docs.keys():\n",
    "        rel_score = 0\n",
    "        for query_word in query:\n",
    "            x = query_word_count[query_word]\n",
    "            y_temp = tf_idf_docs.get(doc_name, {}).get(query_word)\n",
    "            y = y_temp if y_temp else 0\n",
    "            rel_score += x * y \n",
    "        \n",
    "        relevance_scores[doc_name] = rel_score\n",
    "    \n",
    "    return relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m7irt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "NLTK_STOP_WORDS = [word for word in stopwords.words('english')]\n",
    "\n",
    "chars_to_remove = list(string.punctuation)\n",
    "chars_to_remove += [\"*\", \"-\", \"/\", \"+\"]\n",
    "chars_to_remove = set(chars_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Files Found: 22379\n"
     ]
    }
   ],
   "source": [
    "paths = glob(os.path.join('Original_Text_Files', '*.txt'))\n",
    "print(f\"Text Files Found: {len(paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7c0af25c3481ea3dac2d434759f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input_path in tqdm_notebook(paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "    input_path = remove_stop_words(input_path)\n",
    "    input_path = remove_punctuation(input_path)\n",
    "    input_path = to_lowercase(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6748e8e01fd4f8ca2954a60a886adbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_content = get_document_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3a5ae05f94d328b78997437689c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary: 1808540\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(documents_content)\n",
    "print(f\"Words in Vocabulary: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b8a718ead84395991e2d24f1ef999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_term_frequency = get_documnet_term_frequency(documents_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cbac00794e45528f6315a34f017e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_document_frequency = get_word_document_frequency(vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62afe9989394730a7563eed7b5596dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idf_vocabulary = get_idf(vocabulary, len(paths), word_document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2ed1a5611f45e7a9e864ef7ea23b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_documents = get_tf_idf(idf_vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {\n",
    "    1 : \"LDA\",\n",
    "    2 : \"Topic modelling\",\n",
    "    3 : \"Generative models\",\n",
    "    4 : \"Semantic relationships between terms\",\n",
    "    5 : \"Natural Language Prrocessing\",\n",
    "    6 : \"Text Mining\",\n",
    "    7 : \"Translation model\",\n",
    "    8 : \"Learning model\",\n",
    "    9 : \"Semantic evaluations\",\n",
    "    10 : \"System results and combination\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mQuery # 1:\u001b[0m LDA\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ14-2003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  382.13569917748\n",
      "\u001b[1mDocument Name:\u001b[0m \tD09-1026.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  352.7406453945969\n",
      "\u001b[1mDocument Name:\u001b[0m \tD11-1050.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  342.94229413363587\n",
      "\u001b[1mDocument Name:\u001b[0m \tN10-1070.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  313.5472403507528\n",
      "\u001b[1mDocument Name:\u001b[0m \tP10-1117.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  298.84971345931126\n",
      "\n",
      "\u001b[1mQuery # 2:\u001b[0m Topic modelling\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ14-2003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  379.4902612898605\n",
      "\u001b[1mDocument Name:\u001b[0m \tP12-1079.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  338.76447715143644\n",
      "\u001b[1mDocument Name:\u001b[0m \tQ15-1004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  303.5922090318884\n",
      "\u001b[1mDocument Name:\u001b[0m \tN15-1074.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  301.74103702559637\n",
      "\u001b[1mDocument Name:\u001b[0m \tW10-4104.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  281.37814495638435\n",
      "\n",
      "\u001b[1mQuery # 3:\u001b[0m Generative models\n",
      "\u001b[1mDocument Name:\u001b[0m \tW06-1668.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  187.6090498930423\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  176.02977221496937\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ03-4003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  149.81290586100675\n",
      "\u001b[1mDocument Name:\u001b[0m \tD09-1111.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  132.98051273548964\n",
      "\u001b[1mDocument Name:\u001b[0m \tD09-1058.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  132.23411905337565\n",
      "\n",
      "\u001b[1mQuery # 4:\u001b[0m Semantic relationships between terms\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  1043.6132419908004\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ08-2004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  201.8914384556315\n",
      "\u001b[1mDocument Name:\u001b[0m \tW15-3808.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  185.08429317623128\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ91-1003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  184.25659918760599\n",
      "\u001b[1mDocument Name:\u001b[0m \tW09-2004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  178.67830016293192\n",
      "\n",
      "\u001b[1mQuery # 5:\u001b[0m Natural Language Prrocessing\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  106.62161254506168\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ14-1005.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  88.83184861348217\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ87-1020.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  50.62076680018018\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ87-3010.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  37.81375163086052\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ83-2008.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  34.41548944281199\n",
      "\n",
      "\u001b[1mQuery # 6:\u001b[0m Text Mining\n",
      "\u001b[1mDocument Name:\u001b[0m \tD09-1162.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  163.3399312132653\n",
      "\u001b[1mDocument Name:\u001b[0m \tP06-1062.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  163.00619956817914\n",
      "\u001b[1mDocument Name:\u001b[0m \tP12-1062.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  123.78243593621865\n",
      "\u001b[1mDocument Name:\u001b[0m \tW09-2609.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  120.47207251125283\n",
      "\u001b[1mDocument Name:\u001b[0m \tP09-1098.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  111.32268418887817\n",
      "\n",
      "\u001b[1mQuery # 7:\u001b[0m Translation model\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ85-2006.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  448.3393402567721\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ03-3003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  417.55397518506936\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ06-4004.pdf(1).txt\t\t\u001b[1mRelevance Score:\u001b[0m  335.800590614465\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ06-4004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  335.800590614465\n",
      "\u001b[1mDocument Name:\u001b[0m \tW14-3302.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  327.82609124600634\n",
      "\n",
      "\u001b[1mQuery # 8:\u001b[0m Learning model\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  288.9822436503058\n",
      "\u001b[1mDocument Name:\u001b[0m \tP11-5002.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  182.78284854283729\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ15-2004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  105.53245822936898\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ08-3002.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  102.58305895129149\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ03-2004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  100.86885142249105\n",
      "\n",
      "\u001b[1mQuery # 9:\u001b[0m Semantic evaluations\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  863.1392144110445\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ08-2004.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  198.44083825750147\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ09-4008.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  194.29404896579754\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ09-2003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  160.05629655075847\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ91-1003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  123.84446475194434\n",
      "\n",
      "\u001b[1mQuery # 10:\u001b[0m System results and combination\n",
      "\u001b[1mDocument Name:\u001b[0m \tW11-0100.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  292.236184951003\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ01-2002.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  159.90585460395286\n",
      "\u001b[1mDocument Name:\u001b[0m \tN10-1141.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  154.39506893262728\n",
      "\u001b[1mDocument Name:\u001b[0m \tJ11-3003.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  137.11115537403313\n",
      "\u001b[1mDocument Name:\u001b[0m \tP11-1127.pdf.txt\t\t\u001b[1mRelevance Score:\u001b[0m  130.76380174378988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    query = queries[i]\n",
    "    print(f\"\\033[1mQuery # {i}:\\033[0m {query}\")\n",
    "    \n",
    "    query = query.lower()\n",
    "    query = query.split()\n",
    "    relevance_scores = vector_space_model(query, tf_idf_documents)\n",
    "    \n",
    "    sorted_relevance_scores = sorted(relevance_scores, key = relevance_scores.get, reverse = True)\n",
    "    \n",
    "    i = 0\n",
    "    for doc_name in sorted_relevance_scores:\n",
    "        print(f\"\\033[1mDocument Name:\\033[0m \\t{doc_name}\\t\\t\\033[1mRelevance Score:\\033[0m  {relevance_scores[doc_name]}\")\n",
    "        \n",
    "        i += 1\n",
    "        if i == 5: break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
