{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bV8aJD9yo7CD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m7irt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "NLTK_STOP_WORDS = [word for word in stopwords.words('english')]\n",
    "\n",
    "chars_to_remove = list(string.punctuation)\n",
    "chars_to_remove += [\"*\", \"-\", \"/\", \"+\"]\n",
    "chars_to_remove = set(chars_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Files Found: 22379\n"
     ]
    }
   ],
   "source": [
    "paths = glob(os.path.join('Original_Text_Files', '*.txt'))\n",
    "print(f\"Text Files Found: {len(paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(input_file_path):\n",
    "    output_file_path = \"./Stopwords_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                if l_word in NLTK_STOP_WORDS:\n",
    "                    words.remove(word)\n",
    "\n",
    "            output_file.write(\" \".join(words))\n",
    "            output_file.write(\" \")\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_file_path):\n",
    "    output_file_path = \"./Punctuation_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w\") as output_file:\n",
    "        data = input_file.read()\n",
    "        for char in chars_to_remove:\n",
    "            data = data.replace(str(char), \"\")\n",
    "        \n",
    "        output_file.write(data)\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(input_file_path):\n",
    "    output_file_path = \"./Lowercased_Documents/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            lowercased_words = []\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                lowercased_words.append(l_word)\n",
    "\n",
    "            output_file.write(\" \".join(lowercased_words))\n",
    "            output_file.write(\" \")\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7c0af25c3481ea3dac2d434759f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input_path in tqdm_notebook(paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "    input_path = remove_stop_words(input_path)\n",
    "    input_path = remove_punctuation(input_path)\n",
    "    input_path = to_lowercase(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_content():\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    docs_with_content = {}\n",
    "    \n",
    "    for input_path in tqdm_notebook(docs_paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "        doc_words = []\n",
    "        with open(input_path, \"r\") as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            for line in lines:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    doc_words.append(word)\n",
    "        \n",
    "        docs_with_content[os.path.basename(input_path)] = doc_words\n",
    "\n",
    "    return docs_with_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6748e8e01fd4f8ca2954a60a886adbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_content = get_document_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(documents):\n",
    "    vocabulary = {}\n",
    "    for doc_words in tqdm_notebook(documents.values(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        for word in doc_words: \n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = True\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3a5ae05f94d328b78997437689c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary: 1808540\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(documents_content)\n",
    "print(f\"Words in Vocabulary: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documnet_term_frequency(documents_content):\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    doc_term_frequency = {}\n",
    "    \n",
    "    for input_path in docs_paths:\n",
    "        doc_name = os.path.basename(input_path)\n",
    "        doc_term_frequency[doc_name] = {}\n",
    "        \n",
    "    for doc in tqdm_notebook(documents_content.keys(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        current_doc_words = documents_content[doc]\n",
    "        words_count = {}\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] = 0\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] += 1\n",
    "        \n",
    "        doc_term_frequency[doc] = words_count\n",
    "    \n",
    "    return doc_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b8a718ead84395991e2d24f1ef999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_term_frequency = get_documnet_term_frequency(documents_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_document_frequency(vocabulary, document_tf):\n",
    "    word_df = {}\n",
    "    for vocab_word in tqdm_notebook(vocabulary.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        word_doc_count = 0\n",
    "        for tf in document_tf.values():\n",
    "            if vocab_word in tf:\n",
    "                word_doc_count += 1\n",
    "                \n",
    "        word_df[vocab_word] = word_doc_count\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cbac00794e45528f6315a34f017e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_document_frequency = get_word_document_frequency(vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(vocab, m, word_df):\n",
    "    inverse_df_vocab = {}\n",
    "    for vocab_word in tqdm_notebook(vocab.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        inverse_df_vocab[vocab_word] = np.log2((m + 1) / word_df[vocab_word])\n",
    "    \n",
    "    return inverse_df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62afe9989394730a7563eed7b5596dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idf_vocabulary = get_idf(vocabulary, len(paths), word_document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(idf_vocab, doc_tf):\n",
    "    tf_idf_docs = {}\n",
    "    for doc_name in doc_tf.keys():\n",
    "        tf_idf_docs[doc_name] = {}\n",
    "        \n",
    "    for (doc_name, words_freq) in tqdm_notebook(doc_tf.items(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        tf_idf_words = {}\n",
    "        for (doc_word, word_freq) in words_freq.items():\n",
    "            tf_idf_words[doc_word] = word_freq * idf_vocab[doc_word]\n",
    "            \n",
    "        tf_idf_docs[doc_name] = tf_idf_words\n",
    "        \n",
    "    return tf_idf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2ed1a5611f45e7a9e864ef7ea23b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_documents = get_tf_idf(idf_vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAi85czDo7CH"
   },
   "source": [
    "### TF-IDF Weightage Sample Code\n",
    "This is a sample code to give students an idea of how TF-IDF weightage model can be applied to calculate relevance score of documents.\n",
    "\n",
    "The example is taken from **lectures**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZVkz_YRo7CH"
   },
   "source": [
    "**P.S Do not get confused with the answers provided for TF-IDF in lecture 4-5, since the example shown had documents from d1-d5 but in actual the collection was large, hence M and k are actually unknown in that solved solution.** <br>\n",
    "**In this example, M = 5 as it is assumed that the collection has 5 docs only.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cnuaEhqo7CI"
   },
   "source": [
    "<h3 style = 'color:purple;'>Vector Space Model (TF-IDF Weightage Model)</h3>\n",
    "\n",
    "$$ f(q,d) = sim(q,d) =  \\sum_{i=1}^n x_iy_i $$ \n",
    "q = (x_1,.....,x_n) <br>\n",
    "d = (y_1,.....,y_n) <br>\n",
    "x_i = count of word W_i in query. <br>\n",
    "y_i = TF-IDF of word W_i in doc i.e $$ y_i = C(W_i,doc) * log_2 \\frac {M+1} {k} $$\n",
    "M = number of documents in the collection <br>\n",
    "k = document frequency for every word in corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PD0sZeVIo7CJ"
   },
   "outputs": [],
   "source": [
    "#lets say we have the following documents\n",
    "documents = {\n",
    "    \"d1\" : \"news about\",\n",
    "    \"d2\" : \"news about organic food campaign\",\n",
    "    \"d3\" : \"news of presidential campaign\",\n",
    "    \"d4\" : \"news of presidential campaign presidential candidate\",\n",
    "    \"d5\" : \"news of organic food campaign campaign campaign campaign\"\n",
    "} # a dictionary with doc# as key and doc content as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tywh-LX4o7CP",
    "outputId": "8c9f4527-b37e-4908-fba8-8ee43e37fff0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1': 'news about',\n",
       " 'd2': 'news about organic food campaign',\n",
       " 'd3': 'news of presidential campaign',\n",
       " 'd4': 'news of presidential campaign presidential candidate',\n",
       " 'd5': 'news of organic food campaign campaign campaign campaign'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize the dictionary\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC3nCQGAo7CS"
   },
   "outputs": [],
   "source": [
    "#create a corpus ccontaining the vocabulary of words in the documents\n",
    "corpus = [] # a list that will store words of the vocabulary\n",
    "for doc in documents.values(): #iterate through documents \n",
    "    for word in doc.split(): #go through each word in the current doc\n",
    "        if not word in corpus: \n",
    "            corpus.append(word) #add word in corpus if not already added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6BWkOMAo7CU",
    "outputId": "fe359cb1-2097-4a2f-c6c3-67d9cf58b797"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news',\n",
       " 'about',\n",
       " 'organic',\n",
       " 'food',\n",
       " 'campaign',\n",
       " 'of',\n",
       " 'presidential',\n",
       " 'candidate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize the corpus \n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wKx5ximo7CX"
   },
   "outputs": [],
   "source": [
    "#lets create a dictionary that will store document frequency for each word in the corpus\n",
    "df_corpus = {} #document frequency for every word in corpus\n",
    "for word in corpus:\n",
    "    k = 0 #initial document frequency set to 0\n",
    "    for doc in documents.values(): #iterate through documents\n",
    "        if word in doc.split(): #check if word in doc\n",
    "            k+=1 \n",
    "    df_corpus[word] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KUYj1iCo7CZ",
    "outputId": "251b22ec-6b96-4ed9-f9aa-6a30b2191a27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': 5,\n",
       " 'about': 2,\n",
       " 'organic': 2,\n",
       " 'food': 2,\n",
       " 'campaign': 4,\n",
       " 'of': 3,\n",
       " 'presidential': 2,\n",
       " 'candidate': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify the document frequency values\n",
    "df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evrsElsio7Cc"
   },
   "outputs": [],
   "source": [
    "#since we have calculated k (document frequency) for all the words in the corpus, next step is to calculate idf\n",
    "M = len(documents) #number of documents in the collection\n",
    "idf_corpus = {} #inverse_document frequency for every word in corpus\n",
    "for word in corpus:\n",
    "    idf_corpus[word] = np.log2((M+1) / df_corpus[word]) #log_2 ((M+1)/k) i.e inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-zqu-Od0o7Cf",
    "outputId": "8865a3d2-b973-48a5-a3ab-33cee0a7844c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news': 0.2630344058337938,\n",
       " 'about': 1.584962500721156,\n",
       " 'organic': 1.584962500721156,\n",
       " 'food': 1.584962500721156,\n",
       " 'campaign': 0.5849625007211562,\n",
       " 'of': 1.0,\n",
       " 'presidential': 1.584962500721156,\n",
       " 'candidate': 2.584962500721156}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize idf values\n",
    "idf_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksTOOZgeo7Ci"
   },
   "source": [
    "We have successfully calculated inverse_document_frequency for all the words in the corpus. Now, using the idf values, we need to calculate tf-idf for each word with respect to a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbTGhfJuo7Ci"
   },
   "outputs": [],
   "source": [
    "#calculating tf_idf\n",
    "tf_idf_docs = {} #will store tf_idf scores for document words\n",
    "for doc_id in documents.keys():\n",
    "    tf_idf_docs[doc_id] = {} #initialize empty dictionary for each doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wI1-7jRuo7Ck",
    "outputId": "d82acfc2-e144-4953-eac2-8d0940ccf451"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1': {}, 'd2': {}, 'd3': {}, 'd4': {}, 'd5': {}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize tf_idf initial state\n",
    "tf_idf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjIg5jajo7Cn"
   },
   "outputs": [],
   "source": [
    "#finalizing the tf_idf calculations\n",
    "for word in corpus:\n",
    "    for doc_id,doc in documents.items(): #iterate through key,value pairs where key = doc_id and value = doc content\n",
    "        tf_idf_docs[doc_id][word] = doc.split().count(word) * idf_corpus[word] #C(W_i,doc) * IDF(W_i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oz9d1OB4o7Cp",
    "outputId": "709e74d8-f7c8-477f-bec6-465f6cdc8493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d1': {'about': 1.584962500721156,\n",
       "  'campaign': 0.0,\n",
       "  'candidate': 0.0,\n",
       "  'food': 0.0,\n",
       "  'news': 0.2630344058337938,\n",
       "  'of': 0.0,\n",
       "  'organic': 0.0,\n",
       "  'presidential': 0.0},\n",
       " 'd2': {'about': 1.584962500721156,\n",
       "  'campaign': 0.5849625007211562,\n",
       "  'candidate': 0.0,\n",
       "  'food': 1.584962500721156,\n",
       "  'news': 0.2630344058337938,\n",
       "  'of': 0.0,\n",
       "  'organic': 1.584962500721156,\n",
       "  'presidential': 0.0},\n",
       " 'd3': {'about': 0.0,\n",
       "  'campaign': 0.5849625007211562,\n",
       "  'candidate': 0.0,\n",
       "  'food': 0.0,\n",
       "  'news': 0.2630344058337938,\n",
       "  'of': 1.0,\n",
       "  'organic': 0.0,\n",
       "  'presidential': 1.584962500721156},\n",
       " 'd4': {'about': 0.0,\n",
       "  'campaign': 0.5849625007211562,\n",
       "  'candidate': 2.584962500721156,\n",
       "  'food': 0.0,\n",
       "  'news': 0.2630344058337938,\n",
       "  'of': 1.0,\n",
       "  'organic': 0.0,\n",
       "  'presidential': 3.169925001442312},\n",
       " 'd5': {'about': 0.0,\n",
       "  'campaign': 2.3398500028846247,\n",
       "  'candidate': 0.0,\n",
       "  'food': 1.584962500721156,\n",
       "  'news': 0.2630344058337938,\n",
       "  'of': 1.0,\n",
       "  'organic': 1.584962500721156,\n",
       "  'presidential': 0.0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize final tf_idf scores for each doc\n",
    "tf_idf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0qeVZh1Do7Cs"
   },
   "source": [
    "### Querying the documents for relevance scores\n",
    "Since we have calculated the term frequencies for all the documents in our collection, let us calcualte the relevance score of each document for a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "YQbLWueKo7Cs",
    "outputId": "fc1edb76-fb46-42da-9a68-ce63325ee141"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'news about presidential campaign'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"news about presidential campaign\" #the query\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQOo4-Zso7Cv"
   },
   "outputs": [],
   "source": [
    "query_vocab = [] # will store the unique words that occur in the query\n",
    "for word in query.split():\n",
    "    if word not in query_vocab:\n",
    "        query_vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3NqsUaLo7Cx",
    "outputId": "30615ffd-7be6-4294-b4a7-024f7fe804b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news', 'about', 'presidential', 'campaign']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vocab # the unique words in the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-t1ncfxo7Cz"
   },
   "outputs": [],
   "source": [
    "query_wc = {} # a dictionary to store count of a word in the query (i.e x_i according to lecture slides terminology)\n",
    "for word in query_vocab:\n",
    "    query_wc[word] = query.split().count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5V4XU4Yo7C1",
    "outputId": "ff48f7d7-7d84-4edf-8b1c-6085fbfa8bc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': 1, 'campaign': 1, 'news': 1, 'presidential': 1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_wc # the count of each word that occurs in the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxnp-cUXo7C3"
   },
   "outputs": [],
   "source": [
    "relevance_scores = {} # a dictionary that will store the relevance score for each doc\n",
    "# doc_id will be the key and relevance score the value for this dictionary\n",
    "for doc_id in documents.keys():\n",
    "    score = 0 #initialze the score for the doc to 0 at the start\n",
    "    for word in query_vocab:\n",
    "        score += query_wc[word] * tf_idf_docs[doc_id][word] # count of word in query * term_freq of the word\n",
    "    relevance_scores[doc_id] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF1xhuzTo7C5",
    "outputId": "32a06822-d958-46c3-87a9-efe174417961"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Relevancy Scores\n",
      " {'d1': 1.84799690655495, 'd2': 2.432959407276106, 'd3': 2.432959407276106, 'd4': 4.017921907997263, 'd5': 2.6028844087184186}\n"
     ]
    }
   ],
   "source": [
    "# lets print the relevance scores for the query\n",
    "print(\"Document Relevancy Scores\\n\",relevance_scores)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
