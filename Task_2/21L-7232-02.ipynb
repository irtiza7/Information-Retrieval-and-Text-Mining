{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bV8aJD9yo7CD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from glob import glob\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m7irt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "NLTK_STOP_WORDS = [word for word in stopwords.words('english')]\n",
    "\n",
    "chars_to_remove = list(string.punctuation)\n",
    "chars_to_remove += [\"*\", \"-\", \"/\", \"+\"]\n",
    "chars_to_remove = set(chars_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Files Found: 22379\n"
     ]
    }
   ],
   "source": [
    "paths = glob(os.path.join('Original_Text_Files', '*.txt'))\n",
    "print(f\"Text Files Found: {len(paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(input_file_path):\n",
    "    output_file_path = \"./Stopwords_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                if l_word in NLTK_STOP_WORDS:\n",
    "                    words.remove(word)\n",
    "\n",
    "            output_file.write(\" \".join(words))\n",
    "            output_file.write(\" \")\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_file_path):\n",
    "    output_file_path = \"./Punctuation_Removed/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w\") as output_file:\n",
    "        data = input_file.read()\n",
    "        for char in chars_to_remove:\n",
    "            data = data.replace(str(char), \"\")\n",
    "        \n",
    "        output_file.write(data)\n",
    "    \n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(input_file_path):\n",
    "    output_file_path = \"./Lowercased_Documents/\" + os.path.basename(input_file_path)\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w+\") as output_file:\n",
    "        lines = input_file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            lowercased_words = []\n",
    "            for word in words:\n",
    "                l_word = word.lower()\n",
    "                lowercased_words.append(l_word)\n",
    "\n",
    "            output_file.write(\" \".join(lowercased_words))\n",
    "            output_file.write(\" \")\n",
    "\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7c0af25c3481ea3dac2d434759f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for input_path in tqdm_notebook(paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "    input_path = remove_stop_words(input_path)\n",
    "    input_path = remove_punctuation(input_path)\n",
    "    input_path = to_lowercase(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_content():\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    docs_with_content = {}\n",
    "    \n",
    "    for input_path in tqdm_notebook(docs_paths, desc = \"Progress\", ncols = 700, unit = \" Files\"):\n",
    "        doc_words = []\n",
    "        with open(input_path, \"r\") as input_file:\n",
    "            lines = input_file.readlines()\n",
    "            for line in lines:\n",
    "                words = line.split()\n",
    "                for word in words:\n",
    "                    doc_words.append(word)\n",
    "        \n",
    "        docs_with_content[os.path.basename(input_path)] = doc_words\n",
    "\n",
    "    return docs_with_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6748e8e01fd4f8ca2954a60a886adbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "documents_content = get_document_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(documents):\n",
    "    vocabulary = {}\n",
    "    for doc_words in tqdm_notebook(documents.values(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        for word in doc_words: \n",
    "            if word not in vocabulary:\n",
    "                vocabulary[word] = True\n",
    "    \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3a5ae05f94d328b78997437689c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in Vocabulary: 1808540\n"
     ]
    }
   ],
   "source": [
    "vocabulary = create_vocabulary(documents_content)\n",
    "print(f\"Words in Vocabulary: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documnet_term_frequency(documents_content):\n",
    "    docs_paths = glob(os.path.join('Lowercased_Documents', '*.txt'))\n",
    "    doc_term_frequency = {}\n",
    "    \n",
    "    for input_path in docs_paths:\n",
    "        doc_name = os.path.basename(input_path)\n",
    "        doc_term_frequency[doc_name] = {}\n",
    "        \n",
    "    for doc in tqdm_notebook(documents_content.keys(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        current_doc_words = documents_content[doc]\n",
    "        words_count = {}\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] = 0\n",
    "        \n",
    "        for word in current_doc_words:\n",
    "            words_count[word] += 1\n",
    "        \n",
    "        doc_term_frequency[doc] = words_count\n",
    "    \n",
    "    return doc_term_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b8a718ead84395991e2d24f1ef999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "document_term_frequency = get_documnet_term_frequency(documents_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_document_frequency(vocabulary, document_tf):\n",
    "    word_df = {}\n",
    "    for vocab_word in tqdm_notebook(vocabulary.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        word_doc_count = 0\n",
    "        for tf in document_tf.values():\n",
    "            if vocab_word in tf:\n",
    "                word_doc_count += 1\n",
    "                \n",
    "        word_df[vocab_word] = word_doc_count\n",
    "    return word_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cbac00794e45528f6315a34f017e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_document_frequency = get_word_document_frequency(vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf(vocab, m, word_df):\n",
    "    inverse_df_vocab = {}\n",
    "    for vocab_word in tqdm_notebook(vocab.keys(), desc = \"Progress\", ncols = 700, unit = \" Words\"):\n",
    "        inverse_df_vocab[vocab_word] = np.log2((m + 1) / word_df[vocab_word])\n",
    "    \n",
    "    return inverse_df_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62afe9989394730a7563eed7b5596dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idf_vocabulary = get_idf(vocabulary, len(paths), word_document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf(idf_vocab, doc_tf):\n",
    "    tf_idf_docs = {}\n",
    "    for doc_name in doc_tf.keys():\n",
    "        tf_idf_docs[doc_name] = {}\n",
    "        \n",
    "    for (doc_name, words_freq) in tqdm_notebook(doc_tf.items(), desc = \"Progress\", ncols = 700, unit = \" Documents\"):\n",
    "        tf_idf_words = {}\n",
    "        for (doc_word, word_freq) in words_freq.items():\n",
    "            tf_idf_words[doc_word] = word_freq * idf_vocab[doc_word]\n",
    "            \n",
    "        tf_idf_docs[doc_name] = tf_idf_words\n",
    "        \n",
    "    return tf_idf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa2ed1a5611f45e7a9e864ef7ea23b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|                                                                                               …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf_idf_documents = get_tf_idf(idf_vocabulary, document_term_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {\n",
    "    1 : \"LDA\",\n",
    "    2 : \"Topic modelling\",\n",
    "    3 : \"Generative models\",\n",
    "    4 : \"Semantic relationships between terms\",\n",
    "    5 : \"Natural Language Prrocessing\",\n",
    "    6 : \"Text Mining\",\n",
    "    7 : \"Translation model\",\n",
    "    8 : \"Learning model\",\n",
    "    9 : \"Semantic evaluations\",\n",
    "    10 : \"System results and combination\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_space_model(query, tf_idf_docs):\n",
    "    query_word_count = {}\n",
    "    relevance_scores = {}\n",
    "    \n",
    "    for word in query:\n",
    "        query_word_count[word] = 0\n",
    "        \n",
    "    for word in query:\n",
    "        query_word_count[word] += 1\n",
    "    \n",
    "    for doc_name in tf_idf_docs.keys():\n",
    "        rel_score = 0\n",
    "        for query_word in query:\n",
    "            x = query_word_count[query_word]\n",
    "            y_temp = tf_idf_docs.get(doc_name, {}).get(query_word)\n",
    "            y = y_temp if y_temp else 0\n",
    "            rel_score += x * y \n",
    "        \n",
    "        relevance_scores[doc_name] = rel_score\n",
    "    \n",
    "    return relevance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query # 1: LDA\n",
      "Results:\n",
      "J14-2003.pdf.txt 382.13569917748\n",
      "D09-1026.pdf.txt 352.7406453945969\n",
      "D11-1050.pdf.txt 342.94229413363587\n",
      "N10-1070.pdf.txt 313.5472403507528\n",
      "P10-1117.pdf.txt 298.84971345931126\n",
      "\n",
      "Query # 2: Topic modelling\n",
      "Results:\n",
      "J14-2003.pdf.txt 379.4902612898605\n",
      "P12-1079.pdf.txt 338.76447715143644\n",
      "Q15-1004.pdf.txt 303.5922090318884\n",
      "N15-1074.pdf.txt 301.74103702559637\n",
      "W10-4104.pdf.txt 281.37814495638435\n",
      "\n",
      "Query # 3: Generative models\n",
      "Results:\n",
      "W06-1668.pdf.txt 187.6090498930423\n",
      "W11-0100.pdf.txt 176.02977221496937\n",
      "J03-4003.pdf.txt 149.81290586100675\n",
      "D09-1111.pdf.txt 132.98051273548964\n",
      "D09-1058.pdf.txt 132.23411905337565\n",
      "\n",
      "Query # 4: Semantic relationships between terms\n",
      "Results:\n",
      "W11-0100.pdf.txt 1043.6132419908004\n",
      "J08-2004.pdf.txt 201.8914384556315\n",
      "W15-3808.pdf.txt 185.08429317623128\n",
      "J91-1003.pdf.txt 184.25659918760599\n",
      "W09-2004.pdf.txt 178.67830016293192\n",
      "\n",
      "Query # 5: Natural Language Prrocessing\n",
      "Results:\n",
      "W11-0100.pdf.txt 106.62161254506168\n",
      "J14-1005.pdf.txt 88.83184861348217\n",
      "J87-1020.pdf.txt 50.62076680018018\n",
      "J87-3010.pdf.txt 37.81375163086052\n",
      "J83-2008.pdf.txt 34.41548944281199\n",
      "\n",
      "Query # 6: Text Mining\n",
      "Results:\n",
      "D09-1162.pdf.txt 163.3399312132653\n",
      "P06-1062.pdf.txt 163.00619956817914\n",
      "P12-1062.pdf.txt 123.78243593621865\n",
      "W09-2609.pdf.txt 120.47207251125283\n",
      "P09-1098.pdf.txt 111.32268418887817\n",
      "\n",
      "Query # 7: Translation model\n",
      "Results:\n",
      "J85-2006.pdf.txt 448.3393402567721\n",
      "J03-3003.pdf.txt 417.55397518506936\n",
      "J06-4004.pdf(1).txt 335.800590614465\n",
      "J06-4004.pdf.txt 335.800590614465\n",
      "W14-3302.pdf.txt 327.82609124600634\n",
      "\n",
      "Query # 8: Learning model\n",
      "Results:\n",
      "W11-0100.pdf.txt 288.9822436503058\n",
      "P11-5002.pdf.txt 182.78284854283729\n",
      "J15-2004.pdf.txt 105.53245822936898\n",
      "J08-3002.pdf.txt 102.58305895129149\n",
      "J03-2004.pdf.txt 100.86885142249105\n",
      "\n",
      "Query # 9: Semantic evaluations\n",
      "Results:\n",
      "W11-0100.pdf.txt 863.1392144110445\n",
      "J08-2004.pdf.txt 198.44083825750147\n",
      "J09-4008.pdf.txt 194.29404896579754\n",
      "J09-2003.pdf.txt 160.05629655075847\n",
      "J91-1003.pdf.txt 123.84446475194434\n",
      "\n",
      "Query # 10: System results and combination\n",
      "Results:\n",
      "W11-0100.pdf.txt 292.236184951003\n",
      "J01-2002.pdf.txt 159.90585460395286\n",
      "N10-1141.pdf.txt 154.39506893262728\n",
      "J11-3003.pdf.txt 137.11115537403313\n",
      "P11-1127.pdf.txt 130.76380174378988\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    query = queries[i]\n",
    "    print(f\"\\nQuery # {i}: {query}\\nResults:\")\n",
    "    \n",
    "    query = query.lower()\n",
    "    query = query.split()\n",
    "    relevance_scores = vector_space_model(query, tf_idf_documents)\n",
    "    \n",
    "    sorted_scores = sorted(relevance_scores, key = relevance_scores.get, reverse = True)\n",
    "    \n",
    "    i = 0\n",
    "    for doc in sorted_scores:\n",
    "        print(doc, relevance_scores[doc])\n",
    "        i += 1\n",
    "        if i == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
