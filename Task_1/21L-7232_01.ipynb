{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Preprocessing\n",
    "\n",
    "##### Removing punctuation characters\n",
    "##### Removing digits\n",
    "##### Saving output to \"preprocessed_file.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T17:45:39.267037Z",
     "iopub.status.busy": "2022-09-18T17:45:39.266637Z",
     "iopub.status.idle": "2022-09-18T17:45:39.287208Z",
     "shell.execute_reply": "2022-09-18T17:45:39.285994Z",
     "shell.execute_reply.started": "2022-09-18T17:45:39.267005Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "chars_to_remove = [digit for digit in range(10)]\n",
    "chars_to_remove += list(string.punctuation)\n",
    "chars_to_remove += [\"*\", \"-\", \"/\", \"+\"]\n",
    "chars_to_remove = set(chars_to_remove)\n",
    "\n",
    "input_file_path = \"./text_file.txt\"\n",
    "output_file_path = \"./preprocessed_file.txt\"\n",
    "\n",
    "with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w\") as output_file:\n",
    "        data = input_file.read()\n",
    "        for char in chars_to_remove:\n",
    "            data = data.replace(str(char), \"\")\n",
    "        output_file.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erfK8t1iuudL"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "##### 1.1 Write a function that returns number of lines in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T21:19:15.701961Z",
     "iopub.status.busy": "2022-09-05T21:19:15.701620Z",
     "iopub.status.idle": "2022-09-05T21:19:15.707654Z",
     "shell.execute_reply": "2022-09-05T21:19:15.706601Z",
     "shell.execute_reply.started": "2022-09-05T21:19:15.701937Z"
    },
    "id": "gfMPfvKluuD4"
   },
   "outputs": [],
   "source": [
    "def NumOfLines(file_path):    \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # readlines() returns a list of lines and each element ends with \\r\\n \n",
    "        lines = file.readlines()\n",
    "        count = len(lines)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T21:19:19.016934Z",
     "iopub.status.busy": "2022-09-05T21:19:19.016564Z",
     "iopub.status.idle": "2022-09-05T21:19:19.025164Z",
     "shell.execute_reply": "2022-09-05T21:19:19.024173Z",
     "shell.execute_reply.started": "2022-09-05T21:19:19.016907Z"
    },
    "id": "vZK_nMSwz5I_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Lines are  668\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\" \n",
    "print(\"Number of Lines are \", NumOfLines(FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbKMN0PsvTc4"
   },
   "source": [
    "##### 1.2 Write a function that reurns number of words in text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T21:39:27.793261Z",
     "iopub.status.busy": "2022-09-05T21:39:27.792932Z",
     "iopub.status.idle": "2022-09-05T21:39:27.800189Z",
     "shell.execute_reply": "2022-09-05T21:39:27.798714Z",
     "shell.execute_reply.started": "2022-09-05T21:39:27.793237Z"
    },
    "id": "-iWt8XAvuss3"
   },
   "outputs": [],
   "source": [
    "def NumOfWords(file_path):\n",
    "    words = 0\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            words += len(line.split())\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T21:39:30.476174Z",
     "iopub.status.busy": "2022-09-05T21:39:30.475848Z",
     "iopub.status.idle": "2022-09-05T21:39:30.484220Z",
     "shell.execute_reply": "2022-09-05T21:39:30.483260Z",
     "shell.execute_reply.started": "2022-09-05T21:39:30.476149Z"
    },
    "id": "EkHkdf1Svovt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words are  4385\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "print(\"Number of words are \", NumOfWords(FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j33bUTELvtQW"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "##### Write a function that reurns a dataframe with two columns having stop words in one col and their count in the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T23:20:18.469064Z",
     "iopub.status.busy": "2022-09-05T23:20:18.468733Z",
     "iopub.status.idle": "2022-09-05T23:20:18.476855Z",
     "shell.execute_reply": "2022-09-05T23:20:18.476223Z",
     "shell.execute_reply.started": "2022-09-05T23:20:18.469041Z"
    },
    "id": "xEr9ZDyIvsQ-"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def CreateDF(file_path):\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    # Getting a list of NLTK English stop words and iterating each word to convert it from String type to Bytes\n",
    "    NLTK_STOP_WORDS = [bytes(word, 'utf-8') for word in stopwords.words('english')]\n",
    "    \n",
    "    data = {\"stop_words\" : [], \"count\" : []}\n",
    "    \n",
    "    # Dictionary initialized with stopwords as key and 0 as values\n",
    "    count = {key: 0 for key in NLTK_STOP_WORDS}\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            for word in line.split():\n",
    "                if word in NLTK_STOP_WORDS:\n",
    "                    count[word] += 1\n",
    "                    \n",
    "    for key, value in count.items():\n",
    "        if value > 0:\n",
    "            # Converting key from Bytes type to String type\n",
    "            key = str(key, 'utf-8')\n",
    "            data[\"stop_words\"].append(key)\n",
    "            data[\"count\"].append(value)\n",
    "\n",
    "    data_frame = pandas.DataFrame(data)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T23:20:27.051498Z",
     "iopub.status.busy": "2022-09-05T23:20:27.051167Z",
     "iopub.status.idle": "2022-09-05T23:20:27.073915Z",
     "shell.execute_reply": "2022-09-05T23:20:27.072455Z",
     "shell.execute_reply.started": "2022-09-05T23:20:27.051474Z"
    },
    "id": "Q1E80nKiwAos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   stop_words  count\n",
      "34        the    258\n",
      "43         of    155\n",
      "32          a    112\n",
      "57         to    105\n",
      "35        and     98\n",
      "60         in     75\n",
      "21         is     63\n",
      "40         as     43\n",
      "33         an     41\n",
      "18       that     35\n",
      "46        for     34\n",
      "8          it     28\n",
      "25         be     25\n",
      "61         on     25\n",
      "1          we     24\n",
      "45         by     23\n",
      "47       with     22\n",
      "92        can     20\n",
      "22        are     18\n",
      "27       have     17\n",
      "38         or     16\n",
      "58       from     15\n",
      "17       this     13\n",
      "44         at     12\n",
      "28        has     12\n",
      "0           i     12\n",
      "81       such     10\n",
      "79      other      8\n",
      "69       when      8\n",
      "48      about      8\n",
      "77       more      8\n",
      "76       each      8\n",
      "85       same      7\n",
      "39    because      7\n",
      "15      which      7\n",
      "83        not      7\n",
      "87       than      7\n",
      "91          t      6\n",
      "23        was      6\n",
      "86         so      5\n",
      "52     during      5\n",
      "65    further      5\n",
      "5          he      5\n",
      "93       will      5\n",
      "90          s      5\n",
      "26       been      5\n",
      "36        but      5\n",
      "19      these      5\n",
      "11       they      4\n",
      "12       them      4\n",
      "49    between      4\n",
      "84       only      4\n",
      "97          d      4\n",
      "72        how      3\n",
      "68      there      3\n",
      "82         no      3\n",
      "78       most      3\n",
      "59         up      3\n",
      "70      where      3\n",
      "50       into      3\n",
      "9         its      3\n",
      "41      until      3\n",
      "30         do      3\n",
      "31       does      3\n",
      "56      below      2\n",
      "37         if      2\n",
      "94       just      2\n",
      "6         his      2\n",
      "89       very      2\n",
      "88        too      2\n",
      "13      their      2\n",
      "14       what      2\n",
      "16        who      2\n",
      "20      those      2\n",
      "29        had      2\n",
      "80       some      2\n",
      "75       both      2\n",
      "51    through      2\n",
      "62       over      2\n",
      "74        any      2\n",
      "99         ve      2\n",
      "24       were      1\n",
      "63      under      1\n",
      "98          o      1\n",
      "2         our      1\n",
      "96        now      1\n",
      "95     should      1\n",
      "73        all      1\n",
      "3        ours      1\n",
      "4         you      1\n",
      "7         her      1\n",
      "42      while      1\n",
      "64      again      1\n",
      "71        why      1\n",
      "66       then      1\n",
      "10     itself      1\n",
      "67       once      1\n",
      "53     before      1\n",
      "55      above      1\n",
      "54      after      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m7irt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "\n",
    "data_frame = CreateDF(FILE_PATH)\n",
    "data_frame = data_frame.sort_values(by = \"count\", ascending = False)\n",
    "print(data_frame.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRp5uBLKwD1I"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "##### Now generate a txt file named RollnumberWS.txt having all content of the given file but excluding stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "RZul2aEMwI5k"
   },
   "outputs": [],
   "source": [
    "def WithoutSW(input_file_path, output_file_path):    \n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    # Getting a list of NLTK English stop words and iterating each word to convert it from String type to Bytes\n",
    "    NLTK_STOP_WORDS = [bytes(word, 'utf-8') for word in stopwords.words('english')]\n",
    "    \n",
    "    with open(input_file_path, \"r\") as input_file, open(output_file_path, \"w\") as output_file:\n",
    "        data = input_file.read()\n",
    "        for stop_word in NLTK_STOP_WORDS:\n",
    "            data = data.replace(str(stop_word), \"\")\n",
    "        output_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "aAlyQTV2wXJj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\m7irt\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "OUTPUT_FILE_PATH = r\"./21L-7232_WS.txt\"\n",
    "\n",
    "WithoutSW(FILE_PATH, OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQu1uJqDwW00"
   },
   "source": [
    "## Task 4\n",
    "\n",
    "##### Write a function that displays all words having length greater than 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "QS8LuhQvwnbL"
   },
   "outputs": [],
   "source": [
    "def GreaterThan5(file_path):\n",
    "    words_set = set()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            line_words = line.split()\n",
    "            for word in line_words:\n",
    "                if len(word) > 5:\n",
    "                    words_set.add(word)\n",
    "    print(words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "3K4JoLYrws-M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'understand', 'Sample', 'applications', 'exactly', 'Environments', 'argument', 'episode', 'specified', 'considered', 'programming', 'search', 'questions', 'runtime', 'support', 'quantifiers', 'Systems', 'Rovick', 'application', 'backtracking', 'Preconditions', 'Pittsburgh', 'Composition', 'CARMEL', 'onethird', 'accomplish', 'teaches', 'present', 'Architecture', 'detail', 'References', 'definitions', 'example', 'Wooldridge', 'intelligent', 'gestion', 'helping', 'showing', 'Zhouet', 'executed', 'domainindependent', 'Tutoring', 'proposes', 'initiative', 'humancomputer', 'knowledge', 'latter', 'further', 'currently', 'rhetorical', 'Hillsdale', 'manage', 'Introduction', 'execution', 'changing', 'repeating', 'organized', 'problem', 'Domainindep', 'Sidner', 'complete', 'important', 'larger', 'benefits', 'beginning', 'Second', 'differs', 'improvements', 'elevator', 'Conceptual', 'optional', 'Institute', 'Uncertain', 'impossible', 'Figure', 'devised', 'instances', 'teaching', 'Technical', 'vector', 'misses', 'Lehuen', 'operation', 'include', 'recond', 'Answer', 'approach', 'Bayesian', 'Pamela', 'request', 'explain', 'contained', 'Science', 'recent', 'supplied', 'actions', 'referees', 'appropriate', 'finitestate', 'Learning', 'external', 'controls', 'predict', 'described', 'methods', 'categories', 'maintain', 'direction', 'doesnt', 'dialoguebased', 'agenda', 'Further', 'earlier', 'indeed', 'congrs', 'augmented', 'COLINGACL', 'Report', 'transition', 'Machine', 'multiple', 'primitive', 'interactive', 'Conversation', 'information', 'between', 'Baltimore', 'utterances', 'deduce', 'Formalizing', 'covers', 'determining', 'Illinois', 'particular', 'smaller', 'Physics', 'reasons', 'Linguistics', 'respond', 'taskoriented', 'initiate', 'leaved', 'Intelligent', 'simplifies', 'Hierarchical', 'through', 'Berlin', 'written', 'Making', 'solutions', 'Pupils', 'describing', 'naturally', 'opposite', 'Vassileva', 'oriented', 'Artificial', 'natural', 'needed', 'passenger', 'humanhuman', 'becomes', 'levator', 'Fifteenth', 'freedrkpitt', 'Journal', 'performance', 'analogy', 'realworld', 'content', 'including', 'Springer', 'history', 'Analysis', 'Massachusetts', 'especially', 'Student', 'lasAndes', 'planning', 'upperleft', 'replaces', 'transitions', 'account', 'severely', 'Network', 'separate', 'constructs', 'prefer', 'purpose', 'isolated', 'CIRCSIMTutor', 'Harvard', 'ResourceBounded', 'definition', 'Coached', 'straight', 'Unexpected', 'operator', 'velocity', 'Erlbaum', 'during', 'directed', 'Oxford', 'Management', 'initial', 'Twelfth', 'recognize', 'originally', 'specific', 'concerned', 'discuss', 'Designing', 'wrongwith', 'explicitly', 'Island', 'Physiology', 'sequentially', 'Velocity', 'Integrated', 'Theoretical', 'follow', 'largely', 'rendered', 'Freedman', 'states', 'Conati', 'portion', 'connecting', 'following', 'FLAIRS', 'iexical', 'addition', 'making', 'firstorder', 'interleaved', 'Wilkins', 'combinatorially', 'processor', 'Spelling', 'Remember', 'Beliefs', 'Retryat', 'lowerleft', 'correction', 'produces', 'Acknowledgments', 'shared', 'seconds', 'transaction', 'sequences', 'output', 'classical', 'predicates', 'Intelligence', 'According', 'parent', 'International', 'before', 'Teachers', 'generous', 'sufficient', 'styles', 'enabling', 'largest', 'attack', 'professional', 'required', 'reduce', 'educes', 'httpwwwpitt', 'assistance', 'although', 'elements', 'agents', 'condition', 'conversationbased', 'similar', 'project', 'achieving', 'longer', 'intelligence', 'elaborated', 'transcripts', 'Delivering', 'Research', 'Musical', 'allowing', 'Dialogue', 'expected', 'instead', 'control', 'reduction', 'Thirteenth', 'reductio', 'Jordan', 'conditions', 'Coulthard', 'expressions', 'implements', 'coding', 'English', 'cannot', 'meansend', 'Originally', 'majored', 'solving', 'giving', 'artificielle', 'Lowrance', 'experienced', 'kilogram', 'finite', 'expand', 'multistep', 'intent', 'significant', 'different', 'hierarchical', 'matched', 'Desires', 'lefthand', 'replies', 'National', 'Interface', 'Discussion', 'researchers', 'produce', 'contain', 'represent', 'network', 'category', 'conduct', 'reconnaissance', 'Hendler', 'magnitude', 'advance', 'solution', 'PlanBased', 'partially', 'partial', 'tutoring', 'Seventh', 'Department', 'Switching', 'advantages', 'reason', 'Conference', 'Society', 'levels', 'advice', 'shuttle', 'global', 'discourse', 'deceleration', 'selected', 'details', 'achieved', 'answers', 'hintsequence', 'beliefs', 'Planner', 'correctly', 'practical', 'enlarge', 'Communication', 'tutors', 'lexical', 'implemented', 'errors', 'provide', 'amount', 'research', 'intended', 'better', 'rather', 'Florida', 'Solving', 'straightforward', 'question', 'Twentyfirst', 'Attention', 'implicitly', 'nested', 'recipe', 'tripartite', 'assumes', 'encouragement', 'stored', 'Madison', 'subdialogues', 'committed', 'issues', 'mental', 'itself', 'conversation', 'Engine', 'response', 'Reactive', 'Teaching', 'Carolyn', 'remove', 'handlers', 'consists', 'structure', 'database', 'Semantic', 'proposed', 'tended', 'acceleration', 'COLINGth', 'arbitrary', 'improve', 'Approaches', 'Decision', 'Abstract', 'describes', 'informationbased', 'pioneered', 'prunereplace', 'evaluation', 'psychological', 'Computational', 'Baggett', 'Dynamic', 'tracks', 'original', 'preconditions', 'components', 'capable', 'machine', 'communicates', 'HTNstyle', 'Embedded', 'filter', 'CIRCLE', 'Utterances', 'without', 'generality', 'latest', 'library', 'construction', 'handling', 'edufreedrk', 'Gertner', 'modeled', 'useful', 'general', 'sequence', 'taskindependent', 'Although', 'justin', 'change', 'ditions', 'inappropriate', 'Prunereplace', 'computer', 'Association', 'totaling', 'embody', 'desires', 'reified', 'illustrate', 'Structure', 'increase', 'Mentoring', 'common', 'Experimental', 'Medical', 'searches', 'monologue', 'Classical', 'Reason', 'generate', 'occurs', 'follows', 'Sanibel', 'circumstances', 'Responding', 'concepts', 'Michael', 'Development', 'Furthermore', 'Problem', 'Pollack', 'Detroit', 'attempts', 'switching', 'Interpreter', 'Ingrand', 'formed', 'Reasoning', 'schemata', 'restriction', 'subsequent', 'involved', 'reduced', 'language', 'Rennes', 'adopted', 'Please', 'reactive', 'future', 'Transcripts', 'matching', 'represented', 'Background', 'Bratmans', 'philosopher', 'decision', 'ground', 'equivalent', 'frequently', 'current', 'supported', 'Abigail', 'Seattle', 'sessions', 'Morgan', 'establish', 'Robust', 'obtain', 'Computing', 'versions', 'objects', 'flexibly', 'Fourth', 'systems', 'Complexity', 'string', 'either', 'iscourse', 'author', 'Procedural', 'transient', 'abandoned', 'schema', 'inefficient', 'Murray', 'ensure', 'number', 'tudent', 'spelling', 'domains', 'arbitrarily', 'variables', 'Towards', 'corpus', 'speakers', 'architecture', 'Washington', 'utilize', 'showed', 'refers', 'Computer', 'freely', 'covered', 'Acceleratiorl', 'CarnegieMellon', 'importantly', 'eliminated', 'hierarchically', 'generated', 'unchanging', 'illustrated', 'Charlottesville', 'precisely', 'Symposium', 'sources', 'prototype', 'efficiency', 'previous', 'insertion', 'Wenger', 'incorrect', 'instantiated', 'empirical', 'Muller', 'divided', 'University', 'graphs', 'Agents', 'Transient', 'perpendicular', 'dynamique', 'Current', 'formes', 'restores', 'expanded', 'tension', 'albeit', 'beings', 'asserts', 'Ringenberg', 'hierarchy', 'Together', 'buttons', 'necessary', 'powerful', 'situation', 'defined', 'integrated', 'analyzed', 'minimizes', 'expressed', 'variety', 'implement', 'managers', 'comments', 'Intentions', 'London', 'invokes', 'retract', 'American', 'derived', 'planners', 'corner', 'coherent', 'Amherst', 'student', 'picture', 'chosen', 'events', 'working', 'removes', 'Information', 'Reacting', 'demonstrate', 'Sixteenth', 'uninstantiated', 'version', 'Largely', 'BeliefDesire', 'Effective', 'simple', 'Cambridge', 'graphical', 'called', 'answer', 'Earlier', 'dialogues', 'interaction', 'Domaindependent', 'package', 'partner', 'handle', 'intentions', 'active', 'generalized', 'Paradigm', 'Framework', 'contains', 'avelage', 'certain', 'matches', 'realtime', 'bypassing', 'Tutorial', 'access', 'component', 'HTNbased', 'unexpected', 'decomposition', 'encounter', 'Screen', 'examples', 'problems', 'DialogueBased', 'rationale', 'achieve', 'values', 'anguage', 'generating', 'Discourse', 'domain', 'dialogue', 'Bratman', 'relying', 'Library', 'Instructional', 'something', 'VanLehn', 'machines', 'screen', 'expanding', 'decide', 'execute', 'corresponding', 'suited', 'Mohammed', 'describe', 'reasoning', 'Planning', 'provides', 'refine', 'Domaindep', 'alternatives', 'speaker', 'subgoals', 'variable', 'Expressivity', 'choose', 'vertical', 'insight', 'equations', 'Wesley', 'suggests', 'continue', 'elevor', 'prevent', 'Interpretation', 'capability', 'nesting', 'algorithms', 'provided', 'replace', 'physics', 'Georgeff', 'correct', 'Implementation', 'includes', 'retryat', 'reacting', 'Chapter', 'Nicolle', 'another', 'Generating', 'assert', 'spawned', 'Conclusions', 'Finally', 'intention', 'decisionmaking', 'method', 'outputonly', 'process', 'Annual', 'Related', 'changes', 'Correction', 'scratch', 'Practical', 'Context', 'unification', 'remaining', 'Technology', 'Luzzati', 'replacement', 'Strategies', 'formatted', 'context', 'understanding', 'appear', 'hiercx', 'sketch', 'development', 'decelerating', 'whether', 'replaced', 'referring', 'hommemachine', 'Interacting', 'porting', 'existing', 'allows', 'concept', 'conversational', 'Prologlike', 'holding', 'effective', 'Example', 'introduced', 'matters', 'deviates', 'session', 'because', 'Constructive', 'satisfied', 'program', 'realization', 'Exactly', 'Israel', 'indispensible', 'driving', 'Knowledge', 'precon', 'college', 'easier', 'vectors', 'Support', 'multiturn', 'series', 'dealing', 'dixime', 'deriving', 'usually', 'upperright', 'system', 'Proceedings', 'hypothticoexprimental', 'appropriately', 'Extending', 'Agency', 'students', 'providing', 'Montreal', 'responses', 'Cognitive', 'algorithm', 'repeated', 'System', 'operators', 'interface', 'Center', 'Intention', 'specialized', 'cooperatively', 'Education', 'inst¢taneous', 'ComputerBased', 'essential', 'science', 'portions', 'tutorial', 'implementing', 'sample', 'AtlasAndes', 'skilled', 'investigate', 'Thedirection', 'possible', 'adding', 'choice', 'developed', 'Prolog', 'models', 'Orlando', 'Eleventh', 'Jenning', 'tatement', 'generation', 'analogybased', 'Sinclair', 'insufficient', 'building', 'should', 'planner', 'Interdisciplinary', 'coached', 'dropping', 'legibility', 'testing', 'others', 'analysis', 'utterance', 'removed', 'ContextDependent', 'Kaufmann', 'ialogue', 'manager', 'intentionbased', 'graduate', 'textbook', 'Evaluate', 'absurdum', 'downward', 'errorclassifying', 'generalization', 'management'}\n"
     ]
    }
   ],
   "source": [
    "FILE_PATh = r\"./preprocessed_file.txt\"\n",
    "\n",
    "GreaterThan5(FILE_PATh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5Lc-EQuwxRj"
   },
   "source": [
    "## Task 5\n",
    "\n",
    "##### Write a function that returns the count of all words which end at y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "dJzEZ9AFxKul"
   },
   "outputs": [],
   "source": [
    "def EndAty(file_path):\n",
    "    words_ending_with_y = 0\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if word[-1] == \"y\":\n",
    "                    words_ending_with_y += 1\n",
    "    \n",
    "    return words_ending_with_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Hkrp_W4sxPDc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "\n",
    "print(EndAty(FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxvGHnwaxS9u"
   },
   "source": [
    "## Task 6\n",
    "\n",
    "##### Write a function that prints the count of lowercase letters in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "LuCPakz4xYuC"
   },
   "outputs": [],
   "source": [
    "def CountLower(file_path):\n",
    "    lowercase_letter_count = 0\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                for character in word:\n",
    "                    if character.islower():\n",
    "                        lowercase_letter_count += 1\n",
    "    \n",
    "    return lowercase_letter_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7mB_Cx0CxdXb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21290\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "\n",
    "print(CountLower(FILE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHtVLQzHxqDa"
   },
   "source": [
    "## Task 7\n",
    "\n",
    "##### Write a function ZICount() that prints the count of all occurences of Z and I (including small cases z and i too)\n",
    "##### For example: Hi! I am Zenia. It was so good to see you! \n",
    "##### Output: I or i : 4 and Z or z : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "AzuW9kPmxpvS"
   },
   "outputs": [],
   "source": [
    "def ZICount(file_path):\n",
    "    i_count = 0\n",
    "    z_count = 0\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                for char in word:\n",
    "                    if char.lower() == 'i':\n",
    "                        i_count += 1\n",
    "                    elif char.lower() == 'z':\n",
    "                        z_count += 1\n",
    "                        \n",
    "    print(f\"I or i: {i_count}\")\n",
    "    print(f\"Z or z: {z_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "hRqosNj-yE1s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I or i: 1701\n",
      "Z or z: 22\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = r\"./preprocessed_file.txt\"\n",
    "\n",
    "ZICount(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcwK-ALVyHlQ"
   },
   "source": [
    "## Task 8\n",
    "\n",
    "##### Write a function to display the content of a file in descending order (based on the word count)\n",
    "##### Content: I am confused to start it but it won't be done until I give it a try\n",
    "##### Output: I it to a am be confused done give start try until wont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YShxmMDFyLbw"
   },
   "outputs": [],
   "source": [
    "def CountAsc(filename):\n",
    "\n",
    "\n",
    "    #Write your logic here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7Erk1_0yYkm"
   },
   "outputs": [],
   "source": [
    "CountAsc(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjdfcoodyZSl"
   },
   "source": [
    "## Task 9\n",
    "\n",
    "##### Perform lemmatisation on the given file and generate a txt file named Rollnumber_lemmatized.txt having all content of the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uG9Nid_ygSJ"
   },
   "outputs": [],
   "source": [
    "def lemmatize_file(fi):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxkfWjC6ymnd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10\n",
    "\n",
    "##### Perform stemming on the given file and generate a txt file named Rollnumber_stemming.txt having all content of the given file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_file():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYediyB6ynU3"
   },
   "source": [
    "#                             GOOD LUCK!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
